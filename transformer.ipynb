{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_output_embedding = 32\n",
    "d_input_embedding = 32\n",
    "n_sequence_words = 10\n",
    "n_head = 8\n",
    "\n",
    "n_words = 1000\n",
    "\n",
    "embedding_layer = torch.nn.Linear(n_words, d_input_embedding)\n",
    "qkv_projection_layer = torch.nn.Linear(d_input_embedding, 3 * d_output_embedding)\n",
    "mask_matrix = torch.triu(torch.ones(n_sequence_words, n_sequence_words)  * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "position = torch.arange(n_sequence_words).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_output_embedding, 2) * (-math.log(10000.0) / d_output_embedding))\n",
    "pe = torch.zeros(n_sequence_words, d_output_embedding)\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, mask = False):\n",
    "    X += pe\n",
    "    qkv = qkv_projection_layer(X)\n",
    "    batch_size, n_sequence_words, _ = qkv.shape\n",
    "    qkv = qkv.reshape(batch_size, n_sequence_words, n_head, -1) # (batch_size, n_sequence_words, n_head, 3 * d_output_embedding for q + k + v)\n",
    "    qkv = qkv.permute(0, 2, 1, 3) # (batch_size, n_head, n_sequence_words, 3 * d_output_embedding for q + k + v) The calculation is per head\n",
    "\n",
    "    q, k, v = qkv.chunk(3, dim=-1) # (batch_size, n_head, n_sequence_words, d_output_embedding) for q, k, v\n",
    "    scale = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(d_output_embedding) # (batch_size, n_head, n_sequence_words, n_sequence_words)\n",
    "    if mask:\n",
    "        scale = scale + mask_matrix\n",
    "\n",
    "    attention = torch.nn.functional.softmax(scale, dim=-1)\n",
    "    return torch.matmul(attention, v), attention # (batch_size, n_head, n_sequence_words, d_output_embedding) for the output of the self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(8, n_sequence_words, d_input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.4760, 0.5240, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.3123, 0.2832, 0.4046, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.2163, 0.2637, 0.2844, 0.2356, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1672, 0.2236, 0.2188, 0.1913, 0.1990, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1348, 0.1886, 0.2283, 0.1643, 0.1514, 0.1326, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1336, 0.1440, 0.1573, 0.1363, 0.1282, 0.1462, 0.1544, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1212, 0.1312, 0.1512, 0.1356, 0.1025, 0.1240, 0.1341, 0.1001, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0951, 0.1461, 0.1313, 0.1272, 0.1034, 0.0903, 0.0938, 0.1096, 0.1031,\n",
       "         0.0000],\n",
       "        [0.0705, 0.1555, 0.1275, 0.1087, 0.0791, 0.0793, 0.0853, 0.1099, 0.0888,\n",
       "         0.0955]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention(X, True)[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  8.4601e-01,  3.1098e-01,\n",
       "          9.5042e-01,  1.7689e-01,  9.8423e-01,  9.9833e-02,  9.9500e-01,\n",
       "          5.6204e-02,  9.9842e-01,  3.1618e-02,  9.9950e-01,  1.7782e-02,\n",
       "          9.9984e-01,  9.9998e-03,  9.9995e-01,  5.6234e-03,  9.9998e-01,\n",
       "          3.1623e-03,  9.9999e-01,  1.7783e-03,  1.0000e+00,  1.0000e-03,\n",
       "          1.0000e+00,  5.6234e-04,  1.0000e+00,  3.1623e-04,  1.0000e+00,\n",
       "          1.7783e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  4.3146e-01,  5.9113e-01,\n",
       "          8.0658e-01,  3.4821e-01,  9.3742e-01,  1.9867e-01,  9.8007e-01,\n",
       "          1.1223e-01,  9.9368e-01,  6.3203e-02,  9.9800e-01,  3.5558e-02,\n",
       "          9.9937e-01,  1.9999e-02,  9.9980e-01,  1.1247e-02,  9.9994e-01,\n",
       "          6.3245e-03,  9.9998e-01,  3.5566e-03,  9.9999e-01,  2.0000e-03,\n",
       "          1.0000e+00,  1.1247e-03,  1.0000e+00,  6.3246e-04,  1.0000e+00,\n",
       "          3.5566e-04,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  9.9325e-01, -1.1597e-01,  8.1265e-01,\n",
       "          5.8275e-01,  5.0854e-01,  8.6104e-01,  2.9552e-01,  9.5534e-01,\n",
       "          1.6790e-01,  9.8580e-01,  9.4726e-02,  9.9550e-01,  5.3323e-02,\n",
       "          9.9858e-01,  2.9995e-02,  9.9955e-01,  1.6869e-02,  9.9986e-01,\n",
       "          9.4867e-03,  9.9995e-01,  5.3348e-03,  9.9999e-01,  3.0000e-03,\n",
       "          1.0000e+00,  1.6870e-03,  1.0000e+00,  9.4868e-04,  1.0000e+00,\n",
       "          5.3348e-04,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  7.7847e-01, -6.2768e-01,  9.5358e-01,\n",
       "          3.0114e-01,  6.5283e-01,  7.5751e-01,  3.8942e-01,  9.2106e-01,\n",
       "          2.2304e-01,  9.7481e-01,  1.2615e-01,  9.9201e-01,  7.1071e-02,\n",
       "          9.9747e-01,  3.9989e-02,  9.9920e-01,  2.2492e-02,  9.9975e-01,\n",
       "          1.2649e-02,  9.9992e-01,  7.1131e-03,  9.9997e-01,  4.0000e-03,\n",
       "          9.9999e-01,  2.2494e-03,  1.0000e+00,  1.2649e-03,  1.0000e+00,\n",
       "          7.1131e-04,  1.0000e+00],\n",
       "        [-9.5892e-01,  2.8366e-01,  3.2394e-01, -9.4608e-01,  9.9995e-01,\n",
       "         -1.0342e-02,  7.7653e-01,  6.3008e-01,  4.7943e-01,  8.7758e-01,\n",
       "          2.7748e-01,  9.6073e-01,  1.5746e-01,  9.8753e-01,  8.8797e-02,\n",
       "          9.9605e-01,  4.9979e-02,  9.9875e-01,  2.8113e-02,  9.9960e-01,\n",
       "          1.5811e-02,  9.9988e-01,  8.8913e-03,  9.9996e-01,  5.0000e-03,\n",
       "          9.9999e-01,  2.8117e-03,  1.0000e+00,  1.5811e-03,  1.0000e+00,\n",
       "          8.8914e-04,  1.0000e+00],\n",
       "        [-2.7942e-01,  9.6017e-01, -2.3037e-01, -9.7310e-01,  9.4715e-01,\n",
       "         -3.2080e-01,  8.7574e-01,  4.8278e-01,  5.6464e-01,  8.2534e-01,\n",
       "          3.3104e-01,  9.4362e-01,  1.8860e-01,  9.8205e-01,  1.0649e-01,\n",
       "          9.9431e-01,  5.9964e-02,  9.9820e-01,  3.3734e-02,  9.9943e-01,\n",
       "          1.8973e-02,  9.9982e-01,  1.0669e-02,  9.9994e-01,  6.0000e-03,\n",
       "          9.9998e-01,  3.3740e-03,  9.9999e-01,  1.8974e-03,  1.0000e+00,\n",
       "          1.0670e-03,  1.0000e+00],\n",
       "        [ 6.5699e-01,  7.5390e-01, -7.1372e-01, -7.0043e-01,  8.0042e-01,\n",
       "         -5.9944e-01,  9.4733e-01,  3.2026e-01,  6.4422e-01,  7.6484e-01,\n",
       "          3.8355e-01,  9.2352e-01,  2.1956e-01,  9.7560e-01,  1.2416e-01,\n",
       "          9.9226e-01,  6.9943e-02,  9.9755e-01,  3.9354e-02,  9.9923e-01,\n",
       "          2.2134e-02,  9.9976e-01,  1.2448e-02,  9.9992e-01,  6.9999e-03,\n",
       "          9.9998e-01,  3.9364e-03,  9.9999e-01,  2.2136e-03,  1.0000e+00,\n",
       "          1.2448e-03,  1.0000e+00],\n",
       "        [ 9.8936e-01, -1.4550e-01, -9.7726e-01, -2.1204e-01,  5.7432e-01,\n",
       "         -8.1863e-01,  9.8904e-01,  1.4763e-01,  7.1736e-01,  6.9671e-01,\n",
       "          4.3485e-01,  9.0050e-01,  2.5029e-01,  9.6817e-01,  1.4178e-01,\n",
       "          9.8990e-01,  7.9915e-02,  9.9680e-01,  4.4972e-02,  9.9899e-01,\n",
       "          2.5296e-02,  9.9968e-01,  1.4226e-02,  9.9990e-01,  7.9999e-03,\n",
       "          9.9997e-01,  4.4987e-03,  9.9999e-01,  2.5298e-03,  1.0000e+00,\n",
       "          1.4226e-03,  1.0000e+00],\n",
       "        [ 4.1212e-01, -9.1113e-01, -9.3982e-01,  3.4166e-01,  2.9126e-01,\n",
       "         -9.5664e-01,  9.9956e-01, -2.9651e-02,  7.8333e-01,  6.2161e-01,\n",
       "          4.8478e-01,  8.7464e-01,  2.8078e-01,  9.5977e-01,  1.5936e-01,\n",
       "          9.8722e-01,  8.9879e-02,  9.9595e-01,  5.0589e-02,  9.9872e-01,\n",
       "          2.8457e-02,  9.9960e-01,  1.6004e-02,  9.9987e-01,  8.9999e-03,\n",
       "          9.9996e-01,  5.0610e-03,  9.9999e-01,  2.8460e-03,  1.0000e+00,\n",
       "          1.6005e-03,  1.0000e+00]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
