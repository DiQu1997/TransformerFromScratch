{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_output_embedding = 32\n",
    "d_input_embedding = 32\n",
    "n_sequence_words = 10\n",
    "n_head = 8\n",
    "\n",
    "n_words = 1000\n",
    "\n",
    "embedding_layer = torch.nn.Linear(n_words, d_input_embedding)\n",
    "qkv_projection_layer = torch.nn.Linear(d_input_embedding, 3 * d_output_embedding)\n",
    "mask_matrix = torch.triu(torch.ones(n_sequence_words, n_sequence_words)  * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# Positional encoding\n",
    "position = torch.arange(n_sequence_words).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_output_embedding, 2) * (-math.log(10000.0) / d_output_embedding))\n",
    "pe = torch.zeros(n_sequence_words, d_output_embedding)\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    self attention function from the transformer model\n",
    "    Q, K, V: (batch_size, n_head, n_sequence_words, d_output_embedding)\n",
    "    mask: (n_sequence_words, n_sequence_words), None if no mask\n",
    "    return: (batch_size, n_head, n_sequence_words, d_output_embedding) for the output of the self-attention\n",
    "            (batch_size, n_head, n_sequence_words, n_sequence_words) for the attention matrix\n",
    "\n",
    "    Example in matrix with batch size 2, n_head 3, n_sequence_words 2, d_output_embedding 3:\n",
    "    Q (or K, V) = [\n",
    "            // batch 1\n",
    "            [\n",
    "                // head 1\n",
    "                [\n",
    "                    // word 1\n",
    "                    [1, 2, 3],\n",
    "                    // word 2 \n",
    "                    [4, 5, 6]\n",
    "                ],\n",
    "                // head 2\n",
    "                [\n",
    "                    [7, 8, 9],\n",
    "                    [10, 11, 12]\n",
    "                ],\n",
    "            ], \n",
    "            // batch 2\n",
    "            [\n",
    "                [\n",
    "                    [13, 14, 15],\n",
    "                    [16, 17, 18]\n",
    "                ],\n",
    "                [\n",
    "                    [19, 20, 21],\n",
    "                    [22, 23, 24]\n",
    "                ]\n",
    "            ], \n",
    "            [\n",
    "                [\n",
    "                    [13, 14, 15], \n",
    "                    [16, 17, 18]\n",
    "                ],\n",
    "                [\n",
    "                    [19, 20, 21],\n",
    "                    [22, 23, 24]\n",
    "                ]\n",
    "            ]\n",
    "        ]\n",
    "\"\"\"\n",
    "def self_attention(Q, K, V, mask = None):\n",
    "    # Q, K, V: (batch_size, n_head, n_sequence_words, d_output_embedding)\n",
    "    scale = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_output_embedding) # (batch_size, n_head, n_sequence_words, n_sequence_words)\n",
    "    if mask is not None:\n",
    "        scale = scale + mask\n",
    "    attention = torch.nn.functional.softmax(scale, dim=-1)\n",
    "    return torch.matmul(attention, V), attention # (batch_size, n_head, n_sequence_words, d_output_embedding) for the output of the self-attention\n",
    "\n",
    "\n",
    "def Encoder(X, mask = None):\n",
    "    # X: (batch_size, n_sequence_words, d_input_embedding)\n",
    "\n",
    "    # Add positional encoding\n",
    "    X += pe\n",
    "\n",
    "    # Q, K, V projection\n",
    "    qkv = qkv_projection_layer(X)\n",
    "    batch_size, n_sequence_words, _ = qkv.shape\n",
    "    qkv = qkv.reshape(batch_size, n_sequence_words, n_head, -1) # (batch_size, n_sequence_words, n_head, 3 * d_output_embedding for q + k + v)\n",
    "    qkv = qkv.permute(0, 2, 1, 3) # (batch_size, n_head, n_sequence_words, 3 * d_output_embedding for q + k + v) The calculation is per head\n",
    "    q, k, v = qkv.chunk(3, dim=-1) # (batch_size, n_head, n_sequence_words, d_output_embedding) for q, k, v\n",
    "    \n",
    "    return self_attention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(8, n_sequence_words, d_input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1167, 0.0865, 0.0755, 0.1049, 0.0823, 0.1172, 0.1027, 0.1184, 0.1077,\n",
       "         0.0883],\n",
       "        [0.0993, 0.1018, 0.1092, 0.1050, 0.1002, 0.0944, 0.1016, 0.0927, 0.0960,\n",
       "         0.0998],\n",
       "        [0.1075, 0.0850, 0.0703, 0.0971, 0.0916, 0.1137, 0.0983, 0.1220, 0.1154,\n",
       "         0.0992],\n",
       "        [0.1253, 0.0728, 0.0656, 0.0916, 0.0960, 0.1171, 0.0954, 0.1102, 0.1330,\n",
       "         0.0931],\n",
       "        [0.1804, 0.0719, 0.1017, 0.0876, 0.0892, 0.1233, 0.0967, 0.0708, 0.1198,\n",
       "         0.0585],\n",
       "        [0.1660, 0.0745, 0.1105, 0.0926, 0.0971, 0.1104, 0.0979, 0.0665, 0.1186,\n",
       "         0.0660],\n",
       "        [0.1782, 0.0637, 0.0916, 0.1001, 0.0918, 0.1099, 0.0993, 0.0680, 0.1306,\n",
       "         0.0669],\n",
       "        [0.1287, 0.0991, 0.1459, 0.0896, 0.1041, 0.1005, 0.0973, 0.0669, 0.0952,\n",
       "         0.0727],\n",
       "        [0.1688, 0.0673, 0.1163, 0.0951, 0.1086, 0.0957, 0.0961, 0.0543, 0.1274,\n",
       "         0.0703],\n",
       "        [0.1299, 0.0906, 0.1234, 0.0825, 0.1141, 0.1043, 0.0936, 0.0716, 0.1109,\n",
       "         0.0793]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(X, None)[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_matrix\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mEncoder\u001b[0;34m(X, mask)\u001b[0m\n\u001b[1;32m     67\u001b[0m qkv \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# (batch_size, n_head, n_sequence_words, 3 * d_output_embedding for q + k + v) The calculation is per head\u001b[39;00m\n\u001b[1;32m     68\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, n_head, n_sequence_words, d_output_embedding) for q, k, v\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mself_attention\u001b[0;34m(Q, K, V, mask)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mself_attention\u001b[39m(Q, K, V, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Q, K, V: (batch_size, n_head, n_sequence_words, d_output_embedding)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q, K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(d_output_embedding) \u001b[38;5;66;03m# (batch_size, n_head, n_sequence_words, n_sequence_words)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask:\n\u001b[1;32m     52\u001b[0m         scale \u001b[38;5;241m=\u001b[39m scale \u001b[38;5;241m+\u001b[39m mask\n\u001b[1;32m     53\u001b[0m     attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scale, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "Encoder(X, mask_matrix)[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
